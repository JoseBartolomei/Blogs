Bootstrap for data analysis
========================================================

The frequency of a health outcome that I am measuring depart from a normal distribution. Furthermore, common methods for hypothesis testing or confidence interval, among others, based on normality assumptions are not advise.

The bootstrap is a general approach to statistical inference based on building a sampling 
distribution for a statistic by re-sampling from the data at hand. ([Fox, 2000][Fox_2000])
Bootstrap lets us compute estimated standard errors, confidence intervals and hypothesis testing
and is very useful when there is doubt that the usual distributional assumptions and asymptotic 
results are valid and accurate.

In a more down to earth words as express by Dr. Patrick Burns in his web page, [here][Burns_Statistics_Web_Boot], a bootstrap is useful to compute the variability of a statistic.  With one data set we have one statistic but after implement a re-sampling method, such as bootstrap, we obtain the statistic the number of time as number of samples is specified in the procedure.  As a result we can observe the variability (distribution) of the statistic. Furthermore, we do not base our interpretation in just one observation but in multiple observations from the same data.

There are various methods for re-sampling statistics. In our study we decided to use the non-parametric
bootstrapping which allowed, as stated by Fox, to estimate the sampling distribution of a statistic
empirically without making assumptions about the form of the population, and without driving the 
sampling distribution explicitly. ([Fox, 2000][Fox_2000])

One of the most important features of bootstrap is that allows to calculate standard errors for statistics for which we don’t have formulas and to check normality for statistics that theory does not’t easily handle.
In addition, bootstrap distribution is use as a way to estimate the variation in a statistic based on
the original data. ([see Hesterberg Bootstrap companion book chapter in his web site][Hesterberg_Chapt])


Generally bootstrapping follows the same basic steps: ([idre, UCLA][idre_ucla])

1. Re-sample a given data set a specified number of times

2. Calculate a specific statistic from each sample

3. Find the standard deviation of the distribution of that statistic"
 
Bellow I will show the process and results of one of our bootstrapping to observe the variability of 
our outcome of interest.  For this we use the a language and environment for statistical computing 
and graphics named R ([R-project][R_Project]).  R is the **Outcome Project** primary tool for data
management, analytic and reporting.  Reasons for this is topic of another blog. To perform the 
bootstrapping we use the R boot package (Canty, 2012).  A good description of the package can be
found at the 2002 R News vol 2/3 of December 2002. ([here][R_News_2002_Boot])

```{r load_data, echo=FALSE, results='hide'}
rm(list=ls())
load(file = "/media/truecrypt2/ORP/Data/HIC/SSS/Images/post_rxhcu_wt.RData")
```

```{r recode, echo=FALSE, results='hide'}
post_rxhcu_wt$treatment <- ifelse (post_rxhcu_wt$rcat1 == "iclra", 0, 1)
post_rxhcu_wt$gender2 <- ifelse (post_rxhcu_wt$gender == "F", 0, 1)
```

## Bootstrap as perform in one of our studies.

In this example we will obtain the 95% confidence interval of the outpatient visit
mean via bootstrap.  The 95% interval will tell us which will be the values of the mean if we repeat
the study x number of time and is a way to know the distribution of the statistic of interest.

```{r mask_data_names, echo=FALSE, results='hide'}
xdf <- post_rxhcu_wt
xdf$ycol <- xdf$rcat1
xdf$ycol <- ifelse(xdf$ycol == "iclra", "cat1", "cat2")
```
First open R, then is needed to load the desire package
```{r library_boot}
library(boot)
library(parallel)
```
A histogram to the outcome of study is constructed as a way of exploratory data analysis.
Notice that our outcome, outpatient visit, is very skewed.  Furthermore, formulas based on normality assumption to calculate confidence intervals is not supported for this outcome.  This is when the bootstrap come handy. Bootstrap relax some of the conditions needed for traditional inference and to do inference in new settings, set us free from the need for normal data or large samples and also set us free from formulas. ([Hesterberg][Hesterberg_Chapt])
 
```{r histogram, echo=FALSE}
hist(xdf[xdf$ycol == "cat1",]$outpatient, 
					main = "Histogram of outpatient visit for condition X",
					xlab = "Outpatient visits",
					col = "yellow", border = "yellow")

summary(xdf[xdf$ycol == "cat1",]$outpatient)
```

To use the boot function within the boot package a function that calculate the desire statistic is 
needed and design as following:
```{r function4boot}
mean4boot <- function (data, index, maxit){
	data <- data[index,]
	stat <- mean(data[data$ycol == "cat1",]$outpatient)
	stat

}
```
Then the boot function is used to obtain a distribution of the statistic of interest.

```{r boot}
boot.mean <- boot(xdf, statistic = mean4boot,	R = 5000, maxit = 100,
                  ncpus = 4, parallel = "multicore")
```

The print option for boot provide the original statistic, a bias measure and a standard error.

- The original statistic is the calculated mean

- The bootstrap estimate of bias is the difference between the mean of the bootstrap distribution
and the value of the statistic in the original sample.

- The bootstrap standard error of a statistic is the standard deviation of the bootstrap distribution
of that statistic.

```{r print_Boot}
print(boot.mean)
```
An adequate option to evaluate the bootstrap distribution is performing a histogram and a Q-Q plot were you can perceive normality. The bootstrap distribution is use to check normality of the sampling distribution.([Hesterberg][Hesterberg_Chapt]) If the bootstrapped result do not follow a normal distribution a solution is to increase the number of replication or use methods to calculate confidence intervals that account for bias and skweness.

As can be appreciated our 5000 sampling provide a nice normal distribution.
```{r plotBootMean, echo=TRUE, results='markup', fig.width= 15}
plot(boot.mean)
```

If the bootstrap distribution of a statistic shows a normal shape and small bias, we can get a 
confidence interval for the parameter by using the bootstrap standard error and the familiar t 
distribution. ([Hesterberg][Hesterberg_Chapt])  The boot.ci function of the boot packages provides 
an easy way to calculate various confidence interval.

```{r bootMeanCI, message=FALSE, warning=FALSE}
boot.ci(boot.mean, conf = 0.95, type = "all")
```

## In summary as express by Tim Hesterberg ([Hesterberg][Hesterberg_Chapt])

* The bootstrap estimate of the bias of a statistic is the mean of the bootstrap
distribution minus the statistic for the original data. Small bias means that
the bootstrap distribution is centered at the statistic of the original sample
and suggests that the sampling distribution of the statistic is centered at the
population parameter.

* The bootstrap can estimate the sampling distribution, bias, and standard error of a wide variety of
statistics, such as the trimmed mean, whether or not statistical theory tells us about their sampling
distributions.

* If the bootstrap distribution is approximately normal and the bias is small,
we can give a bootstrap t confidence interval, statistic ± t∗ SEboot , for the
parameter. Do not use this t interval if the bootstrap distribution is not normal
or shows substantial bias.

## Remarks
There is a wide array of information about bootstrap in books, journals and internet. The references
I used in this blog are bellow in a Reference section.  As commented in our About blog, our objective is to present our data analysis activities and motivate discussion and intrigue on related topics.  To this extend this is not a bootstrap tutorial.

I hope with this blog motivate you to explore more about bootstrap and implement it in your day
to day task.  Do not hesitate in provide recommendation to this document as well as provide examples
or ideas in a discussion.

## References
John Fox, Bootstrapping Regression Models, Appendix to An R and S-Plus Companion to Applied Regression
[Fox_2000]: https://docs.google.com/viewer?a=v&q=cache:EBG5bKdaHTEJ:cran.r-project.org/doc/contrib/Fox-Companion/appendix-bootstrapping.pdf+cran.r-project.org/doc/contrib/Fox.../appendix-bootstrapping&hl=en&pid=bl&srcid=ADGEESgJxcuKMhQlF-XvDupLOgXoZB98TF_aT1y30jxAeoRuCROm9rELc3MWTpRrwum0aXpUuENxt6LQW2GvZosyENl6Xv5wBEm_hx_0QrddUn5ULs1BD57738WKNrUFTHSF_J7f-t4G&sig=AHIEtbSgX2vhh9NTQgWK0eWplbPq9EK9Dw

Burns Statistics Web Page bootstrap section.
(http://www.burns-stat.com/pages/Tutor/bootstrap_re-sampling.html)
[Burns_Statistics_Web_Boot]: http://www.burns-stat.com/pages/Tutor/bootstrap_resampling.html

Institute for Digital Research and Education, UCLA
(http://statistics.ats.ucla.edu/stat/r/library/bootstrap.htm)
[idre_ucla]: http://statistics.ats.ucla.edu/stat/r/library/bootstrap.htm

 R Core Team (2012). R: A language and environment for statistical computing. R Foundation for Statistical
 Computing, Vienna, Austria. ISBN 3-900051-07-0, URL http://www.R-project.org/. 
 (http://www.R-project.org/)
[R_Project]: http://www.R-project.org/

R News (http://cran.r-project.org/doc/Rnews/)
[R_News_2002_Boot]: http://cran.r-project.org/doc/Rnews/

Angelo Canty and Brian Ripley (2012). boot: Bootstrap R (S-Plus) Functions. R package
version 1.3-7.

Tim Hesterberg, David S. Moore, Shaun Monaghan, Ashley Clipson, and Rachel Epstein., Bootstrap Methods and Permutation Tests (BMPT) written as an introduction to these methods, with a focus on the pedagogical value. 
(https://sites.google.com/a/timhesterberg.net/www/bootstrap)
[Hesterberg_Chapt]: https://sites.google.com/a/timhesterberg.net/www/bootstrap

